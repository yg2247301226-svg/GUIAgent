import re
from typing import List, Dict, Any, Optional
import os
import json
import chromadb
from chromadb.config import Settings
import tiktoken
import openai
from openai import OpenAI

class OpenAIMarkdownVectorDB:
    """ä½¿ç”¨çº¯OpenAI APIçš„Markdownå‘é‡æ•°æ®åº“"""
    
    def __init__(self, 
                 persist_directory: str = "./openai_markdown_db",
                 embedding_model: str = "text-embedding-v4",
                 api_key: Optional[str] = None):
        
        self.persist_directory = persist_directory
        self.embedding_model = embedding_model
        self.api_key = api_key or "sk-1b79273a7a7347349e7ce57275ab0c8c"
        
        if not self.api_key:
            raise ValueError("OpenAI APIå¯†é’¥æœªæä¾›ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(api_key=self.api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        
        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        self.chroma_client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        self.collection = self.chroma_client.get_or_create_collection(
            name="markdown_documents",
            metadata={"description": "Markdownæ–‡æ¡£å‘é‡æ•°æ®åº“"}
        )
        
        # åˆå§‹åŒ–tokenizerç”¨äºè®¡ç®—tokenæ•°é‡
        self.encoding = tiktoken.get_encoding("cl100k_base")
    
    def get_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨OpenAI APIè·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            # æ¸…ç†æ–‡æœ¬
            text = text.replace("\n", " ").strip()
            if len(text) > 8192:  # OpenAIæ¨¡å‹çš„æœ€å¤§tokené™åˆ¶
                text = text[:8192]
            
            response = self.client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            return [0.0] * 1536  # text-embedding-3-smallçš„ç»´åº¦
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.encoding.encode(text))
    
    def split_markdown_content(self, md_content: str) -> List[Dict[str, Any]]:
        """åˆ†å‰²Markdownå†…å®¹ä¸ºå—"""
        lines = md_content.split('\n')
        chunks = []
        current_chunk = []
        current_title = ""
        
        heading_pattern = re.compile(r'^(#{1,6})\s+(.+)$')
        ordered_list_pattern = re.compile(r'^\s*\d+\.\s')
        qa_question_pattern = re.compile(r'^-\s+\*\*Q:')
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # æ£€æŸ¥æ ‡é¢˜
            heading_match = heading_pattern.match(lines[i])
            if heading_match:
                if current_chunk:
                    chunks.append({
                        "title": current_title,
                        "content": "\n".join(current_chunk).strip(),
                        "token_count": self.count_tokens("\n".join(current_chunk).strip())
                    })
                    current_chunk = []
                
                current_title = heading_match.group(2).strip()
                current_chunk = [lines[i]]
                i += 1
                continue
            
            # æ£€æŸ¥æœ‰åºåˆ—è¡¨
            if ordered_list_pattern.match(lines[i]):
                if current_chunk:
                    chunks.append({
                        "title": current_title,
                        "content": "\n".join(current_chunk).strip(),
                        "token_count": self.count_tokens("\n".join(current_chunk).strip())
                    })
                
                list_match = re.match(r'^\s*(\d+\.\s+.+)$', lines[i])
                if list_match:
                    current_title = f"**{list_match.group(1).strip()}**"
                    current_chunk = [lines[i]]
                i += 1
                continue
            
            # æ£€æŸ¥Q&Aé—®é¢˜
            if qa_question_pattern.match(lines[i]):
                if current_chunk:
                    chunks.append({
                        "title": current_title,
                        "content": "\n".join(current_chunk).strip(),
                        "token_count": self.count_tokens("\n".join(current_chunk).strip())
                    })
                
                question_match = re.match(r'^-\s+\*\*(Q:.+)$', lines[i])
                if question_match:
                    current_title = f"- {question_match.group(1).strip()}"
                    current_chunk = [lines[i]]
                i += 1
                continue
            
            if not line and not current_chunk:
                i += 1
                continue
            
            current_chunk.append(lines[i])
            i += 1
        
        if current_chunk:
            chunks.append({
                "title": current_title,
                "content": "\n".join(current_chunk).strip(),
                "token_count": self.count_tokens("\n".join(current_chunk).strip())
            })
        
        return chunks
    
    def read_markdown_file(self, file_path: str) -> str:
        """è¯»å–Markdownæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk') as file:
                return file.read()
    
    def add_markdown_file(self, file_path: str) -> Dict[str, Any]:
        """æ·»åŠ Markdownæ–‡ä»¶åˆ°å‘é‡æ•°æ®åº“"""
        if not os.path.exists(file_path):
            return {"success": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
        try:
            content = self.read_markdown_file(file_path)
            chunks = self.split_markdown_content(content)
            
            if not chunks:
                return {"success": False, "error": "æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•åˆ†å—"}
            
            # å‡†å¤‡æ•°æ®
            documents = []
            metadatas = []
            ids = []
            embeddings = []
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{os.path.basename(file_path)}_{i}"
                
                # è·å–åµŒå…¥å‘é‡
                embedding = self.get_embedding(chunk['content'])
                
                documents.append(chunk['content'])
                metadatas.append({
                    "title": chunk['title'],
                    "source": file_path,
                    "chunk_id": i,
                    "token_count": chunk['token_count'],
                    "file_name": os.path.basename(file_path)
                })
                ids.append(chunk_id)
                embeddings.append(embedding)
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.collection.add(
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            total_tokens = sum(chunk['token_count'] for chunk in chunks)
            
            return {
                "success": True,
                "chunks_processed": len(chunks),
                "total_tokens": total_tokens,
                "file_name": os.path.basename(file_path)
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def search_similar(self, 
                      query: str, 
                      n_results: int = 5,
                      similarity_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢"""
        try:
            # è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
            query_embedding = self.get_embedding(query)
            # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            formatted_results = []
            
            if results['documents'] and results['documents'][0]:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0], 
                    results['metadatas'][0], 
                    results['distances'][0]
                )):
                    similarity = 1 - distance  # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                    
                    if similarity >= similarity_threshold:
                        formatted_results.append({
                            "content": doc,
                            "title": metadata.get("title", ""),
                            "source": metadata.get("source", ""),
                            "similarity": similarity,
                            "distance": distance,
                            "metadata": metadata
                        })
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            formatted_results.sort(key=lambda x: x['similarity'], reverse=True)
            
            return formatted_results
            
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def batch_add_files(self, directory_path: str) -> Dict[str, Any]:
        """æ‰¹é‡æ·»åŠ ç›®å½•ä¸‹çš„æ‰€æœ‰Markdownæ–‡ä»¶"""
        if not os.path.exists(directory_path):
            return {"success": False, "error": f"ç›®å½•ä¸å­˜åœ¨: {directory_path}"}
        
        md_files = []
        for file_name in os.listdir(directory_path):
            if file_name.endswith('.md'):
                md_files.append(os.path.join(directory_path, file_name))
        
        if not md_files:
            return {"success": False, "error": "ç›®å½•ä¸‹æœªæ‰¾åˆ°Markdownæ–‡ä»¶"}
        
        results = {
            "total_files": len(md_files),
            "successful_files": 0,
            "failed_files": 0,
            "total_chunks": 0,
            "file_results": []
        }
        
        for md_file in md_files:
            print(f"å¤„ç†æ–‡ä»¶: {os.path.basename(md_file)}")
            result = self.add_markdown_file(md_file)
            
            if result['success']:
                results['successful_files'] += 1
                results['total_chunks'] += result['chunks_processed']
                results['file_results'].append({
                    "file": os.path.basename(md_file),
                    "status": "success",
                    "chunks": result['chunks_processed']
                })
            else:
                results['failed_files'] += 1
                results['file_results'].append({
                    "file": os.path.basename(md_file),
                    "status": "failed",
                    "error": result['error']
                })
        
        return results
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {
                "document_count": count,
                "persist_directory": self.persist_directory,
                "embedding_model": self.embedding_model
            }
        except Exception as e:
            return {"error": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}"}
    
    def get_all_markdown_files(self) -> List[Dict[str, Any]]:
        """è·å–å‘é‡æ•°æ®åº“ä¸­æ‰€æœ‰Markdownæ–‡ä»¶çš„åˆ—è¡¨"""
        try:
            # è·å–é›†åˆä¸­çš„æ‰€æœ‰æ•°æ®
            all_data = self.collection.get(include=["metadatas", "documents"])
            
            if not all_data or not all_data['metadatas']:
                return []
            
            # æå–å”¯ä¸€çš„æ–‡ä»¶ä¿¡æ¯
            unique_files = {}
            for metadata in all_data['metadatas']:
                file_name = metadata.get("file_name", "")
                source = metadata.get("source", "")
                
                if file_name and file_name not in unique_files:
                    unique_files[file_name] = {
                        "file_name": file_name,
                        "source": source,
                        "chunk_count": 0
                    }
                
                if file_name in unique_files:
                    unique_files[file_name]["chunk_count"] += 1
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            files_list = list(unique_files.values())
            
            # æŒ‰æ–‡ä»¶åæ’åº
            files_list.sort(key=lambda x: x["file_name"])
            
            return files_list
            
        except Exception as e:
            print(f"è·å–Markdownæ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def delete_markdown_file(self, file_name: str) -> Dict[str, Any]:
        """ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤æŒ‡å®šæ–‡ä»¶åçš„æ‰€æœ‰æ•°æ®å—"""
        try:
            # è·å–é›†åˆä¸­çš„æ‰€æœ‰æ•°æ®
            all_data = self.collection.get(include=["metadatas", "ids"])
            
            if not all_data or not all_data['metadatas']:
                return {"success": False, "error": "æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®"}
            
            # æ‰¾å‡ºè¦åˆ é™¤çš„ID
            ids_to_delete = []
            for i, metadata in enumerate(all_data['metadatas']):
                if metadata.get("file_name") == file_name:
                    ids_to_delete.append(all_data['ids'][i])
            
            if not ids_to_delete:
                return {"success": False, "error": f"æœªæ‰¾åˆ°æ–‡ä»¶: {file_name}"}
            
            # åˆ é™¤æ•°æ®
            self.collection.delete(ids=ids_to_delete)
            
            return {
                "success": True,
                "deleted_chunks": len(ids_to_delete),
                "file_name": file_name
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def test_connection(self) -> bool:
        """æµ‹è¯•OpenAIè¿æ¥"""
        try:
            # ç®€å•çš„æµ‹è¯•æŸ¥è¯¢
            test_embedding = self.get_embedding("æµ‹è¯•è¿æ¥")
            return len(test_embedding) > 0
        except Exception as e:
            print(f"OpenAIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

class MarkdownKnowledgeRetriever:
    """MarkdownçŸ¥è¯†æ£€ç´¢å™¨"""
    
    def __init__(self, vector_db: OpenAIMarkdownVectorDB):
        self.vector_db = vector_db
    
    def query(self, 
             question: str, 
             max_results: int = 3,
             min_similarity: float = 0.4) -> Dict[str, Any]:
        """æŸ¥è¯¢ç›¸å…³çŸ¥è¯†"""
        results = self.vector_db.search_similar(question, max_results, min_similarity)
        
        return {
            "question": question,
            "results_found": len(results),
            "min_similarity": min_similarity,
            "results": results
        }
    
    def get_context(self, question: str, max_tokens: int = 2000) -> str:
        """è·å–é—®é¢˜çš„ä¸Šä¸‹æ–‡å†…å®¹"""
        results = self.vector_db.search_similar(question, n_results=5, similarity_threshold=0.6)
        
        context_parts = []
        current_tokens = 0
        
        for result in results:
            content = f"æ ‡é¢˜: {result['title']}\nå†…å®¹: {result['content']}"
            content_tokens = self.vector_db.count_tokens(content)
            
            if current_tokens + content_tokens <= max_tokens:
                context_parts.append(content)
                current_tokens += content_tokens
            else:
                break
        
        return "\n\n".join(context_parts)

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    
    # è®¾ç½®OpenAI APIå¯†é’¥
    api_key = "sk-1b79273a7a7347349e7ce57275ab0c8c"  # æ›¿æ¢ä¸ºæ‚¨çš„å®é™…APIå¯†é’¥
    
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    print("åˆå§‹åŒ–OpenAIå‘é‡æ•°æ®åº“...")
    vector_db = OpenAIMarkdownVectorDB(
        persist_directory="./openai_markdown_db",
        api_key=api_key
    )
    
    # æµ‹è¯•è¿æ¥
    if not vector_db.test_connection():
        print("âŒ OpenAIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥")
        return
    
    print("âœ… OpenAIè¿æ¥æˆåŠŸ")
    
    # å¤„ç†Markdownæ–‡ä»¶
    md_file_path = "C:\\Users\\22473\\Desktop\\è®­ç»ƒè¥\\GUIAgent_tars1\\knowledge_base\\é£ä¹¦äº‘æ–‡æ¡£æ’å…¥è¡¨æ ¼.md"
    
    print(f"å¤„ç†æ–‡ä»¶: {md_file_path}")
    result = vector_db.add_markdown_file(md_file_path)
    
    if result['success']:
        print(f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸï¼Œç”Ÿæˆ {result['chunks_processed']} ä¸ªçŸ¥è¯†å—")
        print(f"ğŸ“Š æ€»tokenæ•°é‡: {result['total_tokens']}")
    else:
        print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {result['error']}")
        return
    
    # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
    stats = vector_db.get_collection_stats()
    print(f"æ•°æ®åº“ç»Ÿè®¡: {stats}")
    
    # æµ‹è¯•æœç´¢
    print("\n" + "="*60)
    print("è¯­ä¹‰æœç´¢æµ‹è¯•")
    print("="*60)
    
    test_queries = [
        "å¦‚ä½•åœ¨é£ä¹¦äº‘æ–‡æ¡£ä¸­æ’å…¥è¡¨æ ¼",
        "è¡¨æ ¼çš„åŸºæœ¬æ“ä½œ",
        "Markdownè¡¨æ ¼è¯­æ³•"
    ]
    
    retriever = MarkdownKnowledgeRetriever(vector_db)
    
    for query in test_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        response = retriever.query(query, min_similarity=0.4)
        
        if response['results_found'] > 0:
            print(f"æ‰¾åˆ° {response['results_found']} ä¸ªç›¸å…³ç»“æœ:")
            for i, result in enumerate(response['results'], 1):
                print(f"{i}. ã€{result['title']}ã€‘")
                print(f"   ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                print(f"   å†…å®¹é¢„è§ˆ: {result['content'][:100]}...")
        else:
            print("æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")

def interactive_search():
    """äº¤äº’å¼æœç´¢ç•Œé¢"""
    api_key = "sk-1b79273a7a7347349e7ce57275ab0c8c"  # æ›¿æ¢ä¸ºæ‚¨çš„å®é™…APIå¯†é’¥
    
    vector_db = OpenAIMarkdownVectorDB(api_key=api_key)
    retriever = MarkdownKnowledgeRetriever(vector_db)
    
    print("\nğŸ¤– OpenAIçŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ")
    print("è¾“å…¥é—®é¢˜è¿›è¡Œæœç´¢ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰")
    
    while True:
        try:
            question = input("\nğŸ’¬ è¯·è¾“å…¥é—®é¢˜: ").strip()
            if question.lower() in ['é€€å‡º', 'exit', 'quit']:
                break
            
            if not question:
                continue
            
            print("ğŸ” æœç´¢ä¸­...")
            response = retriever.query(question, min_similarity=0.4)
            
            if response['results_found'] > 0:
                print(f"\nâœ… æ‰¾åˆ° {response['results_found']} ä¸ªç›¸å…³çŸ¥è¯†ç‚¹:")
                for i, result in enumerate(response['results'], 1):
                    print(f"\n{i}. ã€{result['title']}ã€‘")
                    print(f"   ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                    print(f"   æ¥æº: {result['source']}")
                    print(f"   å†…å®¹: {result['content'][:150]}...")
            else:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ç‚¹")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

# if __name__ == "__main__":
    # è¿è¡Œä¸»ç¤ºä¾‹
    # main()
    
    # è¿è¡Œäº¤äº’å¼æœç´¢ï¼ˆå–æ¶ˆæ³¨é‡Šä½¿ç”¨ï¼‰
    # interactive_search()
    
    # æ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼ˆå–æ¶ˆæ³¨é‡Šä½¿ç”¨ï¼‰
    # vector_db = OpenAIMarkdownVectorDB(api_key="æ‚¨çš„APIå¯†é’¥")
    # batch_result = vector_db.batch_add_files("C:\\path\\to\\your\\markdown\\directory")
    # print("æ‰¹é‡å¤„ç†ç»“æœ:", json.dumps(batch_result, indent=2, ensure_ascii=False))